# -*- coding: utf-8 -*-
# @author: xiaobai
import os
import typing

from pydantic import BaseSettings, AnyHttpUrl, Field

project_desc = """
    ğŸ‰ zerorunner ç®¡ç†å‘˜æ¥å£æ±‡æ€» ğŸ‰
    âœ¨ è´¦å·: admin âœ¨
    âœ¨ å¯†ç : 123456 âœ¨
    âœ¨ æƒé™(scopes): admin âœ¨
"""


class Configs(BaseSettings):
    PROJECT_DESC: str = project_desc  # æè¿°
    PROJECT_VERSION: typing.Union[int, str] = 2.0  # ç‰ˆæœ¬
    BASE_URL: AnyHttpUrl = "http://127.0.0.1:8100"  # å¼€å‘ç¯å¢ƒ

    API_PREFIX: str = "/api"  # æ¥å£å‰ç¼€
    STATIC_DIR: str = 'static'  # é™æ€æ–‡ä»¶ç›®å½•
    GLOBAL_ENCODING: str = 'utf8'  # å…¨å±€ç¼–ç 
    CORS_ORIGINS: typing.List[typing.Any] = ["*"]  # è·¨åŸŸè¯·æ±‚
    WHITE_ROUTER = ["/api/user/login"]  # è·¯ç”±ç™½åå•ï¼Œä¸éœ€è¦é‰´æƒ

    SECRET_KEY: str = "kPBDjVk0o3Y1wLxdODxBpjwEjo7-Euegg4kdnzFIRjc"  # å¯†é’¥(æ¯æ¬¡é‡å¯æœåŠ¡å¯†é’¥éƒ½ä¼šæ”¹å˜, tokenè§£å¯†å¤±è´¥å¯¼è‡´è¿‡æœŸ, å¯è®¾ç½®ä¸ºå¸¸é‡)
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 1  # tokenè¿‡æœŸæ—¶é—´: 60 minutes * 24 hours * 1 days = 1 days

    # redis
    REDIS_URI: str = Field(..., env="REDIS_URI")  # redis

    # DATABASE_URI: str = "sqlite+aiosqlite:///./sql_app.db?check_same_thread=False"  # Sqlite(å¼‚æ­¥)
    DATABASE_URI: str = Field(..., env="MYSQL_DATABASE_URI")  # MySQL(å¼‚æ­¥)
    # DATABASE_URI: str = "postgresql+asyncpg://postgres:123456@localhost:5432/postgres"  # PostgreSQL(å¼‚æ­¥)
    DATABASE_ECHO: bool = False  # æ˜¯å¦æ‰“å°æ•°æ®åº“æ—¥å¿— (å¯çœ‹åˆ°åˆ›å»ºè¡¨ã€è¡¨æ•°æ®å¢åˆ æ”¹æŸ¥çš„ä¿¡æ¯)

    # logger
    LOGGER_DIR: str = "logs"  # æ—¥å¿—æ–‡ä»¶å¤¹å
    LOGGER_NAME: str = 'zerorunner.log'  # æ—¥å¿—æ–‡ä»¶å  (æ—¶é—´æ ¼å¼ {time:YYYY-MM-DD_HH-mm-ss}.log)
    LOGGER_LEVEL: str = 'INFO'  # æ—¥å¿—ç­‰çº§: ['DEBUG' | 'INFO']
    LOGGER_ROTATION: str = "10 MB"  # æ—¥å¿—åˆ†ç‰‡: æŒ‰ æ—¶é—´æ®µ/æ–‡ä»¶å¤§å° åˆ‡åˆ†æ—¥å¿—. ä¾‹å¦‚ ["500 MB" | "12:00" | "1 week"]
    LOGGER_RETENTION: str = "7 days"  # æ—¥å¿—ä¿ç•™çš„æ—¶é—´: è¶…å‡ºå°†åˆ é™¤æœ€æ—©çš„æ—¥å¿—. ä¾‹å¦‚ ["1 days"]

    # dir
    BASEDIR: str = os.path.join(os.path.abspath(os.path.dirname(os.path.dirname(__file__))))

    # celery
    broker_url: str = Field(..., env="CELERY_BROKER_URL")
    result_backend: str = Field(..., env="CELERY_RESULT_BACKEND")
    accept_content: typing.List[str] = ["json"]
    result_serializer: str = "json"
    timezone: str = "Asia/Shanghai"
    enable_utc: bool = False
    # å¹¶å‘å·¥ä½œè¿›ç¨‹/çº¿ç¨‹/ç»¿è‰²çº¿ç¨‹æ‰§è¡Œä»»åŠ¡çš„æ•°é‡ é»˜è®¤10
    worker_concurrency: int = 10
    # ä¸€æ¬¡é¢„å–å¤šå°‘æ¶ˆæ¯ä¹˜ä»¥å¹¶å‘è¿›ç¨‹æ•° é»˜è®¤4
    worker_prefetch_multiplier: int = 4
    # æ± å·¥ä½œè¿›ç¨‹åœ¨è¢«æ–°ä»»åŠ¡æ›¿æ¢ä¹‹å‰å¯ä»¥æ‰§è¡Œçš„æœ€å¤§ä»»åŠ¡æ•°ã€‚é»˜è®¤æ˜¯æ²¡æœ‰é™åˆ¶
    worker_max_tasks_per_child: int = 100
    # è¿æ¥æ± ä¸­å¯ä»¥æ‰“å¼€çš„æœ€å¤§è¿æ¥æ•° é»˜è®¤10
    broker_pool_limit: int = 10
    # ä¼ é€’ç»™åº•å±‚ä¼ è¾“çš„é™„åŠ é€‰é¡¹çš„å­—å…¸ã€‚è®¾ç½®å¯è§æ€§è¶…æ—¶çš„ç¤ºä¾‹ï¼ˆRedis å’Œ SQS ä¼ è¾“æ”¯æŒï¼‰
    result_backend_transport_options: typing.Dict[str, typing.Any] = {'visibility_timeout': 3600}
    include: typing.List[typing.Any] = [
        'celery_worker.tasks.test_case',
        'celery_worker.tasks.common',
    ]
    # task_queues = (
    #     Queue("case", )
    # )
    TEST_FILES_DIR: str = os.path.join(os.path.abspath(os.path.dirname(os.path.dirname(__file__))), 'files')

    task_run_pool: int = 3

    # celery beat
    beat_db_uri: str = Field(..., env="CELERY_BEAT_DB_URL")

    class Config:
        case_sensitive = True  # åŒºåˆ†å¤§å°å†™
        env_file = ".env"


config = Configs()
